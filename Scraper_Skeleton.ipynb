{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT Political Methodology Lab Workshop Series  \n",
    "Spring 2021  \n",
    "Andy Halterman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Overview\n",
    "\n",
    "Most news sites and similar web pages can be scraped through a three step process:\n",
    "\n",
    "1. given a link to an article, extracting and formatting all the needed info from the page\n",
    "2. given an archive-type page of links, finding all the links on the page.\n",
    "3. iterating through each page of archives, scraping all the pages from it, and saving to disk\n",
    "\n",
    "We'll write one function for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests is for general HTTP loading\n",
    "import requests\n",
    "# BeautifulSoup is an HTML parser\n",
    "from bs4 import BeautifulSoup\n",
    "# JSON is a nice format for writing out\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes you'll need the regular expressions library and a date library\n",
    "import re\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write a function here that takes in the URL of an article or page, extracts the information we want from HTML, and structures the output.\n",
    "\n",
    "Python things to learn:\n",
    "\n",
    "- calling methods from objects\n",
    "- what a dictionary looks like\n",
    "- how to define a function\n",
    "\n",
    "HTML things to learn: \n",
    "\n",
    "- Chrome inspector\n",
    "- what HTML tags look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the URL of an article to scrape\n",
    "url = \"https://reliefweb.int/report/afghanistan/more-200-displaced-families-receive-cash-assistance-laghman-province\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download article page and get content\n",
    "page = \n",
    "content = \n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to BeautifulSoup\n",
    "soup = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract body text from page HTML (we'll do this together)\n",
    "body = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract title from page HTML\n",
    "title = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract author from page HTML\n",
    "author = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date from page HTML\n",
    "raw_date = \n",
    "# convert from raw text into a standardized form\n",
    "date = dateutil.parser.parse(raw_date)\n",
    "# Put it into a standard ISO format \n",
    "date = date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it all together!\n",
    "def page_scraper(url):\n",
    "    \"\"\"Function to scrape a page\"\"\"\n",
    "    # Code to download and soupify the page\n",
    "    \n",
    "    # All the code to extract pieces from the HTML\n",
    "    title = \"\"\n",
    "    ...\n",
    "    article = {\n",
    "        \"title\" : title,\n",
    "        \"body\" : body,\n",
    "        \"date\" : date,\n",
    "        \"author\" : author\n",
    "    }\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "page_scraper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it on a different page\n",
    "page_scraper(\"https://reliefweb.int/report/iraq/reconstruction-needed-displaced-iraqis-continue-return-iom-iraq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link getter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need to get the URLS of all the pages we want to scrape. We can do this by finding the directory pages, where the links are on it, and how to get all available directory pages.\n",
    "\n",
    "Python things to learn here:\n",
    "\n",
    "- getting values from dictionaries\n",
    "- for loops and list comprehensions\n",
    "- regex with the `re` library\n",
    "- basic string operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to get all the article links from a single directory page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://reliefweb.int/updates?advanced-search=%28PC13%29_%28F10%29&page=1\"\n",
    "page = \n",
    "content = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find(\"?????\").find_all(\"a\")\n",
    "print(links[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out just the links\n",
    "links = [i['href'] for i in links]\n",
    "links[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uh oh! `links` is full of all sorts of garbage. Is there a term\n",
    "#  that we can search for to reliably pull out article links only?\n",
    "links = [i for i in links if bool(re.search(\"?????\", i))]\n",
    "links[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If these weren't complete URLs (a common occurrance) we could use string operations plus a list comprehension to fix this:\n",
    "#links = [\"https://reliefweb.int/\" + i for i in links]\n",
    "#links[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together into a function that takes in a \"page number\" \n",
    "#  and returns all the links to scrape from it.\n",
    "def page_to_link(page_num):\n",
    "    # how to use .format()\n",
    "    link = \"https://reliefweb.int/updates?advanced-search=%28PC13%29_%28F10%29&page={0}\".format(page_num)\n",
    "    # download the page\n",
    "    # get its content\n",
    "    # soupify\n",
    "    # pull out links\n",
    "    # clean links\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "Now we have a function that'll take a page number for the archive page and return all the links.\n",
    "We have another function that'll take in an article URL and give us the structured content from the page.\n",
    "\n",
    "Let's put them together and download a (small!) range of stories.\n",
    "\n",
    "Note: let's be nice to the UN and not all download the whole thing at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the links we want to scrape\n",
    "all_links = []\n",
    "\n",
    "for num in range(1, 5):\n",
    "    lks = page_to_link(num)\n",
    "    all_links.extend(lks) # extend! not append.\n",
    "    \n",
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content = []\n",
    "\n",
    "for link in all_links[10:20]: # be nice to reliefweb and only get some \n",
    "    try:\n",
    "        content = page_scraper(link)\n",
    "        all_content.append(content) # back to append!\n",
    "    except Exception as e:\n",
    "        # if something goes wrong, keep trucking,\n",
    "        #  but print out the link so we can diagnose it.\n",
    "        print(e)\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving as CSV\n",
    "\n",
    "If you're going to work with your text next in R, a CSV is probably the most useful form to save your text in. To save it as a csv, we will convert into a `DataFrame` using `pandas`, a package for working with data in Python. `pandas` will then let us easily write it out to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "content_df = pd.DataFrame(all_content)\n",
    "content_df.to_csv(\"all_content.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Saving as JSON\n",
    "\n",
    "We can also store is as a JSON file. JSON and dictionaries are almost equivalent, so it's a natural form to save a dict as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"reliefweb.json\"\n",
    "\n",
    "with open(FILENAME, \"w\") as f:\n",
    "    json.dump(all_content, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read it back in\n",
    "\n",
    "If you want to load it back in later to analyze, you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"reliefweb.json\"\n",
    "\n",
    "with open(FILENAME, \"r\") as f:\n",
    "    loaded_content = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it the same?\n",
    "assert loaded_content[4] == all_content[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
