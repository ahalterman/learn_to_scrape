{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT Political Science Methods Series  \n",
    "Fall 2019  \n",
    "Andy Halterman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebooks\n",
    "\n",
    "Jupyter notebooks are a way of writing code, running code, and displaying output in one convenient place. You can write code in code blocks, or write markdown/text in blocks like this. It's often useful to explain what you're doing and finding so when you or someone else picks up the notebook in the future, they'll know what's going on.\n",
    "\n",
    "You can execute code chunks by clicking the cell to run and hitting \"Run\" button on the top bar, or by typing \"shift-enter\". You can always return to a previous code chunk, modify it, and re-run it.\n",
    "\n",
    "You can write math in notebooks, just like in Rmarkdown: $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$\n",
    "\n",
    "Jupyter is great for prototyping, but for more heavy duty use, replication, running on a server, etc., I recommend re-writing the code into a `.py` file that can be called from the command line with `python mycode.py`. This process also gives you a chance to refactor your code, making it more efficient, readable, and dependable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python vs. R\n",
    "\n",
    "Python and R are fairly similar. This is a quick overview of the differences to help you get up to speed.\n",
    "\n",
    "### Importing packages\n",
    "\n",
    "- Importing packages is similar: instead of `[R] library(mypackage)`, do `[Python] import mypackage`.\n",
    "- Python also lets you import specific functions from a package: `from mypackage import cool_function`\n",
    "- You can also rename packages if they're too long: `import numpy as np`\n",
    "- Installing packages is a bit different. In R, you run `install.packages(ggplot2)` from *inside* R. To install packages in Python, you need to open the command line and run the installation command from there: `pip install requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Practice! We need to import:\n",
    "# 1. a library called \"requests\"\n",
    "\n",
    "# 2. a module called \"BeautifulSoup\" from a library called \"bs4\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are all those dots for? (Or, methods, attributes, and namespaces)\n",
    "\n",
    "- Dots have special meaning in Python. It's not like R, where people put dots in all sorts of names.\n",
    "- Python is much more careful about keeping packages' functions attached to the functions. If the `requests` library has a function called `get`, you call it like this `requests.get()`. This reminds you where the `get` function came from and prevents you from overwriting some other package's `get` function.\n",
    "- Python is also more \"object oriented\" than R. Objects often have built in or attached functions, called methods. \n",
    "- These methods are called with a dot notation. Compare: \n",
    "```\n",
    "[R] strsplit(\"Andy Halterman\", \" \")\n",
    "```\n",
    "and\n",
    "\n",
    "```\n",
    "    [Python] \"Andy Halterman\".split(\" \")\n",
    "```\n",
    "\n",
    "- Objects can also have attributes, which are pieces of data attached to an object. Example: `andy.subfields = ['methods', 'comparative']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Practice! Can you figure out how to make a string all upper case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures\n",
    "\n",
    "- Like R's vectors, Python uses a lot of lists. These are ordered arrays. Note that Python starts with 0! \n",
    "\n",
    "```\n",
    "my_list = [\"x\", \"y\", \"z\"]\n",
    "> my_list[0] \n",
    "\"x\"\n",
    "```\n",
    "\n",
    "- Python has a data structure called a *dictionary*, which are like lists that you access by key name instead of by position (think a more general form of R's dataframes). Example:\n",
    "\n",
    "```\n",
    "article = {\"title\": \"Rivalry and Revenge\",\n",
    "           \"author\" : \"Balcells\",\n",
    "           \"year\" : \"2017\"}\n",
    "\n",
    "> article.keys()\n",
    "['title', 'author', 'year']\n",
    "\n",
    "> article['author']\n",
    "\"Balcells\"\n",
    "```\n",
    "\n",
    "### Loops and functions\n",
    "\n",
    "\n",
    "- Functions are only slightly different from R:\n",
    "\n",
    "```\n",
    "def my_function(x):\n",
    "    z = (x + 2)^2\n",
    "    return z\n",
    "```\n",
    "\n",
    "- Loops are fast and nice in Python, unlike in R, and are very easily done:\n",
    "\n",
    "```\n",
    "for i in my_list:\n",
    "    my_function(i)\n",
    "```\n",
    "\n",
    "- Pro move: list comprehensions:\n",
    "\n",
    "```\n",
    "[my_function(i) for i in my_list]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Practice! Can you write a loop that goes over a list of words and prints a capitalized version of each one?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whitespace\n",
    "\n",
    "- As you can tell, Python makes heavy use of whitespace to set apart different levels of functions, for loops, etc. Use four spaces (Jupyter converts tabs to four spaces automatically. \n",
    "- No need for curly braces!\n",
    "\n",
    "```\n",
    "def my_function(big_list):\n",
    "    print(len(big_list))\n",
    "    for l in big_list:\n",
    "        for i in l:\n",
    "            ...\n",
    "    return stuff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Overview\n",
    "\n",
    "Most news sites and similar web pages can be scraped through a three step process:\n",
    "\n",
    "1. given a link to an article, extracting and formatting all the needed info from the page\n",
    "2. given an archive-type page of links, finding all the links on the page.\n",
    "3. iterating through each page of archives, scraping all the pages from it, and saving to disk\n",
    "\n",
    "We'll write one function for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests is for general HTTP loading\n",
    "import requests\n",
    "# BeautifulSoup is an HTML parser\n",
    "from bs4 import BeautifulSoup\n",
    "# JSON is a nice format for writing out\n",
    "# ujson can handle datetimes better and is a drop in replacement for the json module\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes you'll need the regular expressions library and a date library\n",
    "import re\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write a function here that takes in the URL of an article or page, extracts the information we want from HTML, and structures the output.\n",
    "\n",
    "Python things to learn:\n",
    "\n",
    "- calling methods from objects\n",
    "- what a dictionary looks like\n",
    "- how to define a function\n",
    "\n",
    "HTML things to learn: \n",
    "\n",
    "- Chrome inspector\n",
    "- what HTML tags look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the URL of an article to scrape\n",
    "url = \"https://reliefweb.int/report/afghanistan/more-200-displaced-families-receive-cash-assistance-laghman-province\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download article page and get content\n",
    "page = \n",
    "content = \n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to BeautifulSoup\n",
    "soup = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract body text from page HTML (we'll do this together)\n",
    "body = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract title from page HTML\n",
    "title = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract author from page HTML\n",
    "author = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date from page HTML\n",
    "raw_date = \n",
    "# convert from raw text into a standardized form\n",
    "date = dateutil.parser.parse(raw_date)\n",
    "# Put it into a standard ISO format \n",
    "date = date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it all together!\n",
    "def page_scraper(url):\n",
    "    \"\"\"Function to scrape a page\"\"\"\n",
    "    # Code to download and soupify the page\n",
    "    \n",
    "    # All the code to extract pieces from the HTML\n",
    "    title = \"\"\n",
    "    ...\n",
    "    article = {\n",
    "        \"title\" : title,\n",
    "        \"body\" : body,\n",
    "        \"date\" : date,\n",
    "        \"author\" : author\n",
    "    }\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "page_scraper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it on a different page\n",
    "page_scraper(\"https://reliefweb.int/report/iraq/reconstruction-needed-displaced-iraqis-continue-return-iom-iraq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link getter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need to get the URLS of all the pages we want to scrape. We can do this by finding the directory pages, where the links are on it, and how to get all available directory pages.\n",
    "\n",
    "Python things to learn here:\n",
    "\n",
    "- getting values from dictionaries\n",
    "- for loops and list comprehensions\n",
    "- regex with the `re` library\n",
    "- basic string operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to get all the article links from a single directory page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://reliefweb.int/country/afg?format=8&page=1\"\n",
    "page = \n",
    "content = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find(\"?????\").find_all(\"a\")\n",
    "print(links[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out just the links\n",
    "links = [i['href'] for i in links]\n",
    "links[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uh oh! `links` is full of all sorts of garbage. Is there a term\n",
    "#  that we can search for to reliably pull out article links only?\n",
    "links = [i for i in links if bool(re.match(\"?????\", i))]\n",
    "links[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These aren't complete urls! We can use string operations plus a list comprehension to fix this:\n",
    "links = [\"https://reliefweb.int/\" + i for i in links]\n",
    "links[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together into a function that takes in a \"page number\" \n",
    "#  and returns all the links to scrape from it.\n",
    "def page_to_link(page_num):\n",
    "    # how to use .format()\n",
    "    link = \"https://reliefweb.int/country/afg?format=8&page={0}#content\".format(page_num)\n",
    "    # download the page\n",
    "    # get its content\n",
    "    # soupify\n",
    "    # pull out links\n",
    "    # clean links\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "Now we have a function that'll take a page number for the archive page and return all the links.\n",
    "We have another function that'll take in an article URL and give us the structured content from the page.\n",
    "\n",
    "Let's put them together and download a (small!) range of stories.\n",
    "\n",
    "Note: let's be nice to the UN and not all download the whole thing at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the links we want to scrape\n",
    "all_links = []\n",
    "\n",
    "for num in range(1, 5):\n",
    "    lks = page_to_link(num)\n",
    "    all_links.extend(lks) # extend! not append.\n",
    "    \n",
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content = []\n",
    "\n",
    "for link in all_links[10:20]: # be nice to reliefweb and only get some \n",
    "    try:\n",
    "        content = page_scraper(link)\n",
    "        all_content.append(content) # back to append!\n",
    "    except Exception as e:\n",
    "        # if something goes wrong, keep trucking,\n",
    "        #  but print out the link so we can diagnose it.\n",
    "        print(e)\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving as CSV\n",
    "\n",
    "If you're going to work with your text next in R, a CSV is probably the most useful form to save your text in. To save it as a csv, we will convert into a `DataFrame` using `pandas`, a package for working with data in Python. `pandas` will then let us easily write it out to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "content_df = pd.DataFrame(all_content)\n",
    "content_df.to_csv(\"all_content.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Saving as JSON\n",
    "\n",
    "We can also store is as a JSON file. JSON and dictionaries are almost equivalent, so it's a natural form to save a dict as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"reliefweb.json\"\n",
    "\n",
    "with open(FILENAME, \"w\") as f:\n",
    "    json.dump(all_content, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read it back in\n",
    "\n",
    "If you want to load it back in later to analyze, you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"reliefweb.json\"\n",
    "\n",
    "with open(FILENAME, \"r\") as f:\n",
    "    loaded_content = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it the same?\n",
    "assert loaded_content[4] == all_content[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
